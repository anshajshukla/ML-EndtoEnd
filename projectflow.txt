Building Pipeline:
1 → Create a GitHub repo and clone it in local (Add experiments)
2 → Add src folder along with all components (run them individually)
3 → Add data, models, reports directories to the .gitignore file
4 → Now git add, commit, push


Setting up dcv Pipeline (without params)
5 --> create dvc.yaml file and add stages to it.
6 --> dvc init then do "dvc repro" to test the pipeline automation. (check dvc dag)
7 --> git add, comit, push


Setting up dcv pipeline (with params)
8 --> add params.yaml file
9 --> Add the params setup (mentioned bhow)
10 --> Do "dvc repro" again to test the pipeline along with the params
11 --> Now git add, comit push


Experiments with DVC:
12 -->pip install dvclive
13 --> Add the dvclive block (entioned belcv)
14 --> Do "dvc exp run", it will create a new dvc.yaml(if already not there) arui dvclive directory (each run will be considered as an experiment)
15 --> do "dvc exp show" on terminal to see the experiments use extensions on VSCode (install dvc extensions)
16 --> Do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
17 --> Change params, rerun code (produce new experiments)
18 --> git add, comit, push


Adding a remote S3 storage to DVC:
19 --> Login to AWS console
20 --> Create an IAM user (straight forward process)
21 --> Create s3 (enter unique naæ and create)
22 --> pip install dvc[s3]
23 --> pip install awscli
24 --> aws configure
25 --> dvc remte add -d dvcstore s3://bucketname
26 --> dvc commit-push the exp outcome that you want to keep
27 --> Finally git add, commit, push